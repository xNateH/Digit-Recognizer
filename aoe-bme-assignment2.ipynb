{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-14T06:49:08.139720Z","iopub.status.busy":"2023-09-14T06:49:08.139276Z","iopub.status.idle":"2023-09-14T06:49:08.150906Z","shell.execute_reply":"2023-09-14T06:49:08.149484Z","shell.execute_reply.started":"2023-09-14T06:49:08.139679Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.optim import Adam, SGD\n","from torch.utils.data import TensorDataset, DataLoader\n","\n","import tensorflow.keras.datasets.mnist as MNIST\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix, accuracy_score\n","import warnings\n","\n","cuda = torch.cuda.is_available()\n","print(\"GPU available:\", cuda)\n","\n","torch.manual_seed(1102)\n","np.random.seed(1102)\n","\n","#References: https://www.kaggle.com/code/snnaik/aoe-bme-lecture3-partc-mlp-lenetformnist"]},{"cell_type":"markdown","metadata":{},"source":["#### a. Downloads the MNIST dataset using MNIST package. (Make sure the images and labels for both training set and test set are downloaded correctly)\n","#### b. Using `train_test_split` function, splits training set into a new training set and validation set by a ratio of 20%.\n","#### c. Prints the number of samples, minimum and maximum intensity, and the shape of the images matrix for all three sets."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-14T06:49:08.153873Z","iopub.status.busy":"2023-09-14T06:49:08.153177Z","iopub.status.idle":"2023-09-14T06:49:08.590415Z","shell.execute_reply":"2023-09-14T06:49:08.589430Z","shell.execute_reply.started":"2023-09-14T06:49:08.153829Z"},"trusted":true},"outputs":[],"source":["(trainingSET, label_train), (testing, label_test) = MNIST.load_data()\n","\n","#split the training set into new training set and validation set\n","trainingSET, validation, label_train, label_val = train_test_split(trainingSET, label_train, test_size=0.2)\n","\n","#print num entries, max and min value, shape\n","print(trainingSET.shape[0], np.max(trainingSET), np.min(trainingSET), trainingSET.shape)\n","print(validation.shape[0], np.max(validation), np.min(validation), validation.shape)\n","print(testing.shape[0], np.max(testing), np.min(testing), testing.shape)"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"markdown","metadata":{},"source":["#### Displays 24 first samples pulled from each set (train, validation, and test) and show the true labels as the title per each sample. \n","**Note:** Displays all 24 samples pulled from each set in one plot as in four rows and six columns. Thus, we will expect three plots for training, validation, and test set."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-14T06:49:08.592490Z","iopub.status.busy":"2023-09-14T06:49:08.591886Z","iopub.status.idle":"2023-09-14T06:49:12.043292Z","shell.execute_reply":"2023-09-14T06:49:12.042334Z","shell.execute_reply.started":"2023-09-14T06:49:08.592444Z"},"trusted":true},"outputs":[],"source":["sets = [trainingSET, trainingSET, trainingSET]\n","labels = [label_train, label_val, label_test]\n","titles = ['Training ', 'Validation', 'Testing']\n","for s in range(len(sets)): \n","    row = 4\n","    col = 6\n","    fig = plt.figure(figsize = (12, 15))\n","    \n","    plt.suptitle(f\"From {titles[s]}\")\n","    for i in range(1, row * col + 1):\n","        \n","        IMG = sets[s][i, :, :]\n","        val = fig.add_subplot(row, col, i)\n","        val.set_xticks([])\n","        val.set_yticks([])\n","        val.title.set_text(str(labels[s][i]))\n","        plt.imshow(IMG, cmap = 'gray')\n","        \n","    plt.show()\n","    print\n"]},{"cell_type":"markdown","metadata":{},"source":["#### a. Reformats all samples and creates a `DataLoader` of batch size 100 for each set. \n","#### b. Prints the total number of batches in each `DataLoader` using `len()` function\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-14T06:49:12.045513Z","iopub.status.busy":"2023-09-14T06:49:12.044892Z","iopub.status.idle":"2023-09-14T06:49:12.201598Z","shell.execute_reply":"2023-09-14T06:49:12.200481Z","shell.execute_reply.started":"2023-09-14T06:49:12.045467Z"},"trusted":true},"outputs":[],"source":["# [your code here]\n","image_train_torch = torch.from_numpy(trainingSET).type(torch.FloatTensor).view(-1, 1, 28, 28)\n","label_train_torch = torch.from_numpy(label_train).type(torch.LongTensor)\n","\n","image_validation_torch = torch.from_numpy(validation).type(torch.FloatTensor).view(-1, 1, 28, 28)\n","label_validation_torch = torch.from_numpy(label_val).type(torch.LongTensor)\n","\n","image_test_torch = torch.from_numpy(testing).type(torch.FloatTensor).view(-1, 1, 28, 28)\n","label_test_torch = torch.from_numpy(label_test).type(torch.LongTensor)\n","\n","train_data = TensorDataset(image_train_torch, label_train_torch)\n","train_loader = DataLoader(train_data, batch_size = 100)\n","\n","print(len(train_loader))"]},{"cell_type":"markdown","metadata":{},"source":["#### </span> Defines a model as the following table:\n","| Layer Number | Layer Type   | Number of kernels | Kernel size | Activation | Stride | Zero-Padding |\n","| ------------ | ------------ | ----------------- | ----------- | ---------- | ------ | ------------ |\n","| 1            | Conv2D       | 32                | 5x5         | ReLU       | 1      | 0            |\n","| 2            | MaxPooling2D | NA                | 2x2         | NA         | 2      | 0            |\n","| 3            | Conv2D       | 64                | 3x3         | ReLU       | 1      | 0            |\n","| 4            | MaxPooling2D | NA                | 2x2         | NA         | 2      | 0            |\n","| 5            | Conv2D       | 128               | 2x2         | ReLU       | 1      | 0            |\n","| 6            | MaxPooling2D | NA                | 2x2         | NA         | 2      | 0            |\n","| 7            | Faltten      | NA                | NA          | NA         | NA     | NA           |\n","| 8            | FC (Linear)  | 128               | NA          | ReLU       | NA     | NA           |\n","| 9            | FC (Linear)  | 64                | NA          | Sigmoid    | NA     | NA           |\n","| 10           | FC (Linear)  | 10                | NA          | SoftMax    | NA     | NA           |\n","**Note:** **NA** means not applicable.\n","\n","#### Creates an instance of the model and load it on the GPU by calling the `.cuda()` function."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-14T06:49:12.207303Z","iopub.status.busy":"2023-09-14T06:49:12.206753Z","iopub.status.idle":"2023-09-14T06:49:12.221558Z","shell.execute_reply":"2023-09-14T06:49:12.220778Z","shell.execute_reply.started":"2023-09-14T06:49:12.207263Z"},"trusted":true},"outputs":[],"source":["# Formula to calculate shape as we go through layer by layer = [(X - F + 2P)/S] + 1\n","# Here,\n","# X = Width / Height\n","# F = Kernel size\n","# P = Padding\n","# S = Strides (default = 1)\n","\n","# Our input to the first layer is going to be [batchsize, 1, 32, 32]\n","# substitute, =[(28 - 5 + 2(0))/1] + 1\n","#             =[(23)/1] + 1\n","#             =23 + 1\n","#             =24\n","\n","\n","class MLPModel(nn.Module):\n","    '''Reshape -> FC -> Sigmoid -> FC -> Sigmoid -> FC -> SoftMax -> Cross-Entropy'''\n","    def __init__(self):\n","        '''Define model modules.'''\n","        super(MLPModel, self).__init__()\n","        self.fc1 = nn.Linear(28 * 28, 128)\n","        self.fc2 = nn.Linear(128, 64)\n","        self.fc3 = nn.Linear(64, 10)\n","\n","    def forward(self, x):\n","        '''Define the model architecture (the sequence to place the model modules).'''\n","        x = x.view(-1, 28 * 28)\n","        x = self.fc1(x)\n","        x = F.sigmoid(x)\n","        x = self.fc2(x)\n","        x = F.sigmoid(x)\n","        x = self.fc3(x)\n","        return F.log_softmax(x, dim = 1)\n","\n","our_MLP = MLPModel()\n","\n","# If GPU available, move the model to GPU.\n","if cuda:\n","    our_MLP.cuda()\n","\n","print(our_MLP)"]},{"cell_type":"markdown","metadata":{},"source":["#### </span> Writes the shape of the output tensor for each layer of the model, knowing the input shape of each image in the MNIST dataset is $28\\times28$.\n","**Note:** Uses the following formula: \\\n","Assuming the input shape is: $(B, Ch_{in}, W_{in}, H_{in})$, and the output shape is $(B, Ch_{out}, W_{out}, H_{out})$ \\\n","1. For the `Conv2D` layers with $N$ number of $K\\times K$ kernels, zero-padding $P$, and stride $S$:\n","$$\n","W_{out} \\text{ or } H_{out} = \\lfloor {\\frac{W_{in} \\text{ or } H_{in} + 2P - K }{S}} + 1 \\rfloor\n","$$\n","$\\lfloor . \\rfloor$ means integer floor.\n","\n","And the number of channels for the output will be $Ch_{out} = N$\n","\n","2. Similarly for the `Maxpooling` layers of size $K \\times K$, zero-padding $P$, and stride $S$:\n","$$\n","W_{out} \\text{ or } H_{out} = \\lfloor {\\frac{W_{in} \\text{ or } H_{in} + 2P - K }{S}} + 1 \\rfloor\n","$$\n","\n","The only difference is that the number of channels for the output will be $Ch_{out} = Ch_{in}$\n","\n","3. For `Linear` (Also known as Fully-Connected) layers, the input is a vector of $(B, D_{in})$. Assuming the layer has $N$ number of neurons, the output is a vector of size $(B, N)$.\n","\n","**Note:** $B$ stands for the batch size."]},{"cell_type":"markdown","metadata":{},"source":["\n","Layer 1: W = [(28 - 5 + 2*0)/1] + 1 = 24, 32 Channels\n","\n","Layer 2: W = [(24 - 2 + 2*0)/2] + 1 = 12, 32 Channels\n","\n","Layer 3: W = [(12 - 3 + 2*0)/1] + 1 = 10, 64 Channels\n","\n","Layer 4: W = [(10 - 2 + 2*0)/2] + 1 = 5, 64 Channels\n","\n","Layer 5: W = [(5 - 2 + 2*0)/1] + 1 = 4, 128 Channels\n","\n","Layer 6: W = [(4 - 2 + 2*0)/2] + 1 = 2, 128 Channels\n","\n","Layer 7: D = 512 *Reshape*\n","\n","Layer 8: Vector: <100, 128>\n","\n","Layer 9: Vector: <100, 64>\n","\n","Layer 10: Vector: <100, 10>"]},{"cell_type":"markdown","metadata":{},"source":["#### </span> Defines an SGD optimizer with a learning rate of $10^{-3}$ on the parameters of the model."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-14T06:49:12.223966Z","iopub.status.busy":"2023-09-14T06:49:12.223462Z","iopub.status.idle":"2023-09-14T06:49:12.229101Z","shell.execute_reply":"2023-09-14T06:49:12.228063Z","shell.execute_reply.started":"2023-09-14T06:49:12.223923Z"},"trusted":true},"outputs":[],"source":["# define our optimizer\n","optimizer = SGD(our_MLP.parameters(), lr = 0.001)"]},{"cell_type":"markdown","metadata":{},"source":["#### </span> Trains the network on a train set for 50 epochs. Uses the \"Cross-Entropy\" (`F.cross_entropy`) as the loss function. \n","#### At the end of each epoch, saves the model, tests the model on the validation set, and keeps the training and validation loss for later. \n","**Note:** Loss is calculated for each batch; thus, average loss needed to be collected over all batches as the epoch loss for both training and validation phases."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-14T06:49:12.231259Z","iopub.status.busy":"2023-09-14T06:49:12.230819Z","iopub.status.idle":"2023-09-14T06:49:13.269721Z","shell.execute_reply":"2023-09-14T06:49:13.268455Z","shell.execute_reply.started":"2023-09-14T06:49:12.231223Z"},"trusted":true},"outputs":[],"source":["!mkdir saved_models_MLP"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-14T06:49:13.272168Z","iopub.status.busy":"2023-09-14T06:49:13.271723Z","iopub.status.idle":"2023-09-14T06:53:55.210163Z","shell.execute_reply":"2023-09-14T06:53:55.209161Z","shell.execute_reply.started":"2023-09-14T06:49:13.272125Z"},"trusted":true},"outputs":[],"source":["EPOCHS = 50\n","\n","train_epoch_loss = []\n","validation_epoch_loss = []\n","\n","for epoch in range(EPOCHS):\n","    train_loss = []\n","    validation_loss = []\n","\n","    for batch_index, (train_image, train_label) in enumerate(train_loader):\n","        # If GPU is available, move the data to the GPU for faster computation.\n","        if cuda:\n","            #######################################################\n","            ####################### Train #########################\n","            #######################################################\n","            # Set the model to train mode so that the parameters can be updated.\n","            our_MLP.train()\n","\n","            train_label_predicted = our_MLP(train_image.cuda())\n","\n","            # compute the loss\n","            loss = F.cross_entropy(train_label_predicted, train_label.cuda())\n","            train_loss.append(loss.cpu().data.item())\n","\n","            # reset the gradient \n","            optimizer.zero_grad()\n","            # backpropagate the loss\n","            loss.backward()\n","            # update the parameters\n","            optimizer.step()\n","\n","            #######################################################\n","            ###################### Validation #####################\n","            #######################################################\n","            # Set the model to evaluation mode so that parameters are fixed.\n","            our_MLP.eval()\n","\n","            validation_label_predicted = our_MLP(image_validation_torch.cuda())\n","\n","            loss = F.cross_entropy(validation_label_predicted, label_validation_torch.cuda())\n","            validation_loss.append(loss.cpu().data.item())\n","        \n","        # If GPU is not available.\n","        else:\n","            #######################################################\n","            ####################### Train #########################\n","            #######################################################\n","            # Set the model to train mode so that the parameters can be updated.\n","            our_MLP.train()\n","\n","            train_label_predicted = our_MLP(train_image)\n","\n","            # compute the loss\n","            loss = F.cross_entropy(train_label_predicted, train_label)\n","            train_loss.append(loss.cpu().data.item())\n","\n","            # reset the gradient\n","            optimizer.zero_grad()\n","            # backpropagate the loss\n","            loss.backward()\n","            # update the parameters\n","            optimizer.step()\n","\n","            #######################################################\n","            ###################### Validation #####################\n","            #######################################################\n","            # Set the model to evaluation mode so that parameters are fixed.\n","            our_MLP.eval()\n","\n","            validation_label_predicted = our_MLP(image_validation_torch)\n","\n","            loss = F.cross_entropy(validation_label_predicted, label_validation_torch)\n","            validation_loss.append(loss.cpu().data.item())\n","\n","    train_epoch_loss.append(np.mean(train_loss))\n","    validation_epoch_loss.append(np.mean(validation_loss))\n","    \n","    # save models\n","    torch.save(our_MLP.state_dict(), './saved_models_MLP/checkpoint_epoch_%s.pth' % (epoch))\n","\n","    print(\"Epoch: {} | train_loss: {} | validation_loss: {}\".format(epoch, train_epoch_loss[-1], validation_epoch_loss[-1]))"]},{"cell_type":"markdown","metadata":{},"source":["#### </span> Plots the learning curve and indicates in which epoch the model achieved the lowest validation loss.\n","**Note:** The learning curve is a plot that shows the training and validation loss for each epoch."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-14T06:53:55.211862Z","iopub.status.busy":"2023-09-14T06:53:55.211492Z","iopub.status.idle":"2023-09-14T06:53:55.431702Z","shell.execute_reply":"2023-09-14T06:53:55.430082Z","shell.execute_reply.started":"2023-09-14T06:53:55.211818Z"},"trusted":true},"outputs":[],"source":["plt.figure(figsize = (12, 8))\n","plt.plot(train_epoch_loss, '-o', label = 'training loss', markersize = 3)\n","plt.plot(validation_epoch_loss, '-o', label = 'validation loss', markersize = 3)\n","plt.legend(loc = 'upper right');\n","\n","\n","best_epoch = np.argmin(validation_epoch_loss)\n","print('best epoch: ', best_epoch)\n"]},{"cell_type":"markdown","metadata":{},"source":["#### a. Loads the weights of the best epoch and tests the model on the test set.\n","#### b. Tests the model on the first sample of the test set and plots the output probabilities.\n","#### c. Prints the overall accuracy of the model on the test set. Accuracy formula is $Accuracy=\\frac{\\text{Number of correct prediction}}{\\text{Total Number of samples}}$\n","#### d. Plots the confusion matrix for the test set.\n","\n","**Note:** The testing phase is similar to the validation phase. You need to loop through batches of the test loader and keep the true labels and predicted labels for accuracy and confusion matrix."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-14T06:53:55.433431Z","iopub.status.busy":"2023-09-14T06:53:55.433055Z","iopub.status.idle":"2023-09-14T06:53:55.446247Z","shell.execute_reply":"2023-09-14T06:53:55.445114Z","shell.execute_reply.started":"2023-09-14T06:53:55.433393Z"},"trusted":true},"outputs":[],"source":["state_dict = torch.load('./saved_models_MLP/checkpoint_epoch_%s.pth' % (epoch))\n","print(state_dict.keys())\n","our_MLP.load_state_dict(state_dict)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-14T06:53:55.448939Z","iopub.status.busy":"2023-09-14T06:53:55.448667Z","iopub.status.idle":"2023-09-14T06:53:55.659611Z","shell.execute_reply":"2023-09-14T06:53:55.658716Z","shell.execute_reply.started":"2023-09-14T06:53:55.448912Z"},"trusted":true},"outputs":[],"source":["def predict_with_pytorch(model, input_data):\n","    model.eval()\n","    label_predicted_all = []\n","\n","    label_predicted_one_hot = model(input_data)\n","    label_predicted_probability, label_predicted_index = torch.max(label_predicted_one_hot.data, 1)\n","    \n","    for current_prediction in label_predicted_index:\n","        label_predicted_all.append(current_prediction.detach().cpu().numpy().item())\n","\n","    return label_predicted_all\n","\n","if cuda:\n","    test_label_predicted = predict_with_pytorch(our_MLP, image_test_torch.cuda())\n","else:\n","    test_label_predicted = predict_with_pytorch(our_MLP, image_test_torch)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-14T06:53:55.661749Z","iopub.status.busy":"2023-09-14T06:53:55.661241Z","iopub.status.idle":"2023-09-14T06:53:55.919100Z","shell.execute_reply":"2023-09-14T06:53:55.918048Z","shell.execute_reply.started":"2023-09-14T06:53:55.661705Z"},"trusted":true},"outputs":[],"source":["def view_classify(image, probabilities, version = \"MNIST\"):\n","    ''' Function for viewing an image and it's predicted classes.\n","    '''\n","    probabilities = probabilities.data.numpy().squeeze()\n","\n","    fig, (ax1, ax2) = plt.subplots(figsize = (6, 9), ncols = 2)\n","    ax1.imshow(image.resize_(1, 28, 28).numpy().squeeze(), cmap = 'gray')\n","    ax1.set_title('Original Image')\n","    ax1.axis('off')\n","    ax2.bar(np.arange(10), probabilities)\n","    ax2.set_aspect(10)\n","    ax2.set_xticks(np.arange(10))\n","        \n","    ax2.set_title('Class Probability')\n","    ax2.set_ylim(0, 1.1)\n","\n","    plt.tight_layout()\n","   \n","our_MLP.eval()\n","\n","sample_test_image = image_test_torch[0, :, :, :][np.newaxis, :, :, :]\n","\n","if cuda:\n","    sample_prediction = our_MLP(sample_test_image.cuda())\n","else:\n","    sample_prediction = our_MLP(sample_test_image)\n","    \n","warnings.filterwarnings('ignore')\n","\n","if cuda:\n","    view_classify(sample_test_image, 2 ** sample_prediction.cpu())\n","else:\n","    view_classify(sample_test_image, 2 ** sample_prediction)\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-14T06:53:55.920840Z","iopub.status.busy":"2023-09-14T06:53:55.920454Z","iopub.status.idle":"2023-09-14T06:53:55.960593Z","shell.execute_reply":"2023-09-14T06:53:55.959711Z","shell.execute_reply.started":"2023-09-14T06:53:55.920801Z"},"trusted":true},"outputs":[],"source":["j=0\n","match = 0\n","\n","for i in test_label_predicted:\n","    if i== label_test[j]:\n","        match = match + 1\n","    j = j+1\n","    \n","    \n","print(\"Accuracy:\", round((match/len(label_test) * 100),2) , \"%\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-14T06:53:55.962391Z","iopub.status.busy":"2023-09-14T06:53:55.961994Z","iopub.status.idle":"2023-09-14T06:53:56.968054Z","shell.execute_reply":"2023-09-14T06:53:56.967011Z","shell.execute_reply.started":"2023-09-14T06:53:55.962353Z"},"trusted":true},"outputs":[],"source":["CM = confusion_matrix(label_test, test_label_predicted)\n","\n","plt.figure(figsize = (12,10))\n","sns.heatmap(CM, annot = True, annot_kws = {\"size\": 10})\n","plt.ylim([0, 10]);\n","plt.ylabel('True labels');\n","plt.xlabel('predicted labels');"]},{"cell_type":"markdown","metadata":{},"source":["#### Final Thoughts:"]},{"cell_type":"markdown","metadata":{},"source":["My model performed better on the MNIST classification compared to the LeNet model. I would presume a larger network of more layers would raise accuracy.\n","To improve the accuracy of either model, I would suggest using a bigger set of data/sources. Also, I would calculate the batch size and the optimal learning rate."]}],"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"},"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"}},"nbformat":4,"nbformat_minor":4}
